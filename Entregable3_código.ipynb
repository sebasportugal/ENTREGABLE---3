{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "id": "-MVX3mONZ2S6",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7421e5b2-5519-46d5-e8ec-d4eaf2e682a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357289 sha256=c6edd13b926f81b63bcf7f42a1c5aa6d5a4a996fb3d289ea4a6c2fbb59766b54\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD, KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from sklearn.metrics import average_precision_score"
      ],
      "metadata": {
        "id": "wwZdX_2YYbso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carga y Exploración de Datos**"
      ],
      "metadata": {
        "id": "hBF9ca2zbCYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    metadata = pd.read_csv('metadata.csv', usecols=['asset_id', 'content_id', 'end_vod_date'])\n",
        "    train = pd.read_csv('train.csv', usecols=['account_id', 'asset_id', 'tunein'])\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error al cargar los archivos CSV: {e}\")\n",
        "    raise\n",
        "except pd.errors.EmptyDataError as e:\n",
        "    print(f\"El archivo CSV está vacío: {e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error inesperado al cargar los datos: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "kDXz3MYCa-RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación de columnas y muestra de datos\n",
        "print(\"Metadata:\")\n",
        "print(metadata.head())\n",
        "print(metadata.columns)\n",
        "\n",
        "print(\"\\nTrain:\")\n",
        "print(train.head())\n",
        "print(train.columns)"
      ],
      "metadata": {
        "id": "f_7J95eorn0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversión de fechas a formato datetime\n",
        "try:\n",
        "    metadata['end_vod_date'] = pd.to_datetime(metadata['end_vod_date'], errors='coerce')\n",
        "    train['tunein'] = pd.to_datetime(train['tunein'], errors='coerce')\n",
        "except Exception as e:\n",
        "    print(f\"Error al convertir las fechas: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "_wThmvzHcPcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**División del Dataset en Train y Test**"
      ],
      "metadata": {
        "id": "QgAeDsGTeymn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    train_df = train[train['tunein'] < '2021-03-01']\n",
        "    test_df = train[train['tunein'] >= '2021-03-01']\n",
        "except KeyError as e:\n",
        "    print(f\"Error en la división de los datos: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "EbtZV1FVutZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generación de Ratings Binarios**"
      ],
      "metadata": {
        "id": "_vPa4iz3fAwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generación de ratings binarios\n",
        "train_df['rating'] = 1"
      ],
      "metadata": {
        "id": "sMo-8Y_GchAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos contenidos disponibles después del 1 de marzo de 2021\n",
        "try:\n",
        "    available_content = metadata[metadata['end_vod_date'] >= '2021-03-01']['content_id'].unique()\n",
        "except KeyError as e:\n",
        "    print(f\"Error al filtrar contenido disponible: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "wRhDpEW2eAq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación de valores nulos y conversión de tipos de datos\n",
        "print(\"Valores nulos en 'asset_id' de train_df:\", train_df['asset_id'].isnull().sum())\n",
        "print(\"Valores nulos en 'asset_id' de metadata:\", metadata['asset_id'].isnull().sum())\n",
        "\n",
        "train_df = train_df[train_df['asset_id'].notnull()]\n",
        "metadata = metadata[metadata['asset_id'].notnull()]"
      ],
      "metadata": {
        "id": "rx_sz4GaOQnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    train_df['asset_id'] = train_df['asset_id'].astype(int)\n",
        "    metadata['asset_id'] = metadata['asset_id'].astype(int)\n",
        "except ValueError as e:\n",
        "    print(f\"Error al convertir 'asset_id' a entero: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "Hmm84HIVADZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unimos train_df con metadata para obtener 'content_id'\n",
        "try:\n",
        "    train_df = pd.merge(train_df, metadata[['asset_id', 'content_id']], on='asset_id', how='left')\n",
        "except KeyError as e:\n",
        "    print(f\"Error al hacer el merge de los datos: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "yT-y9_saObZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpieza de columnas duplicadas en train_df\n",
        "train_df = train_df.loc[:, ~train_df.columns.duplicated()]"
      ],
      "metadata": {
        "id": "Z8uuS4eTOdu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación después del merge\n",
        "print(\"\\nColumnas en train_df después del merge:\")\n",
        "print(train_df.columns)\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "odVM6t3cOgsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'content_id' not in train_df.columns:\n",
        "    print(\"Error: La columna 'content_id' no se creó en el merge.\")\n",
        "else:\n",
        "    print(\"\\nTrain DataFrame después del merge:\")\n",
        "    print(train_df.head())"
      ],
      "metadata": {
        "id": "pyDylJaSZ04s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos por contenido disponible\n",
        "try:\n",
        "    train_df = train_df[train_df['content_id'].isin(available_content)]\n",
        "except KeyError as e:\n",
        "    print(f\"Error al filtrar el DataFrame por contenido disponible: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "rsjD2QWhfqYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unimos test_df con metadata para obtener 'content_id'\n",
        "try:\n",
        "    test_df = pd.merge(test_df, metadata[['asset_id', 'content_id']], on='asset_id', how='left')\n",
        "except KeyError as e:\n",
        "    print(f\"Error al hacer el merge del test set con metadata: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "oSt9eM4kZ_KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpieza de columnas duplicadas en test_df\n",
        "test_df = test_df.loc[:, ~test_df.columns.duplicated()]"
      ],
      "metadata": {
        "id": "HOUKvlJvAo61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación después del merge en test_df\n",
        "print(\"\\nColumnas en test_df después del merge:\")\n",
        "print(test_df.columns)\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "id": "t9OwQBBrAv6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Factorización de Matrices (SVD)**"
      ],
      "metadata": {
        "id": "-bkeqwNof5yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica que 'content_id' esté presente en train_df\n",
        "print(\"\\nColumnas en train_df antes de cargar datos en Surprise:\")\n",
        "print(train_df.columns)"
      ],
      "metadata": {
        "id": "itnIDcyuf8xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    reader = Reader(rating_scale=(1, 1))\n",
        "    data = Dataset.load_from_df(train_df[['account_id', 'content_id', 'rating']], reader)\n",
        "except KeyError as e:\n",
        "    print(f\"Error al cargar los datos en Surprise: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "sLHNBq66gCgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos los datos en train y test sets\n",
        "try:\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "except Exception as e:\n",
        "    print(f\"Error al dividir el dataset en train/test: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "MwNn85JBgLDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo SVD\n",
        "try:\n",
        "    svd = SVD()\n",
        "    svd.fit(trainset)\n",
        "except Exception as e:\n",
        "    print(f\"Error al entrenar el modelo SVD: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "15WRRcCQghCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecimos y evaluamos el modelo\n",
        "try:\n",
        "    predictions = svd.test(testset)\n",
        "    accuracy.rmse(predictions)\n",
        "except Exception as e:\n",
        "    print(f\"Error al realizar predicciones con SVD: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "QX4WSe_lRxpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generación de Recomendaciones**"
      ],
      "metadata": {
        "id": "i6KR55uHgpat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_recomendaciones_svd(model, account_id, train_df, num_recomendaciones=20):\n",
        "    try:\n",
        "        vistos = set(train_df[train_df['account_id'] == account_id]['content_id'].values)\n",
        "        all_items = train_df['content_id'].unique()\n",
        "\n",
        "        recomendaciones = [(item, model.predict(account_id, item).est) for item in all_items if item not in vistos]\n",
        "        recomendaciones = sorted(recomendaciones, key=lambda x: x[1], reverse=True)[:num_recomendaciones]\n",
        "\n",
        "        return [rec[0] for rec in recomendaciones]\n",
        "    except KeyError as e:\n",
        "        print(f\"Error en la generación de recomendaciones: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "KXXA2FYXg308"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manejo de usuarios 'cold start'\n",
        "def generar_recomendaciones_cold_start(train_df, num_recomendaciones=20):\n",
        "    try:\n",
        "        contenido_popular = train_df['content_id'].value_counts().index[:num_recomendaciones]\n",
        "        return contenido_popular.tolist()\n",
        "    except KeyError as e:\n",
        "        print(f\"Error en la generación de recomendaciones para cold start: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "owNzAC4fSFpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de recomendaciones para un usuario\n",
        "try:\n",
        "    account_id_ejemplo = test_df['account_id'].iloc[0]\n",
        "    if account_id_ejemplo in train_df['account_id'].values:\n",
        "        recomendaciones = generar_recomendaciones_svd(svd, account_id_ejemplo, train_df)\n",
        "    else:\n",
        "        print(f\"Usuario {account_id_ejemplo} es un caso de cold start.\")\n",
        "        recomendaciones = generar_recomendaciones_cold_start(train_df)\n",
        "    print(f\"Recomendaciones para el usuario {account_id_ejemplo}: {recomendaciones}\")\n",
        "except IndexError as e:\n",
        "    print(f\"Error al intentar obtener el primer account_id de test_df: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "9kZQVwuchvXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generación de recomendaciones para todos los usuarios en el test set\n",
        "def verificar_recomendaciones(model, test_df, train_df, num_recomendaciones=20):\n",
        "    recomendaciones_por_usuario = {}\n",
        "\n",
        "    for account_id in test_df['account_id'].unique():\n",
        "        if account_id in train_df['account_id'].values:\n",
        "            recomendaciones = generar_recomendaciones_svd(model, account_id, train_df, num_recomendaciones)\n",
        "        else:\n",
        "            print(f\"Usuario {account_id} es un caso de cold start.\")\n",
        "            recomendaciones = generar_recomendaciones_cold_start(train_df, num_recomendaciones)\n",
        "\n",
        "        recomendaciones_por_usuario[account_id] = recomendaciones\n",
        "\n",
        "    return recomendaciones_por_usuario\n",
        "\n",
        "try:\n",
        "    recomendaciones_por_usuario = verificar_recomendaciones(svd, test_df, train_df)\n",
        "    for user, recs in recomendaciones_por_usuario.items():\n",
        "        print(f\"Recomendaciones para el usuario {user}: {recs}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al generar recomendaciones para todos los usuarios: {e}\")"
      ],
      "metadata": {
        "id": "4gOApbOgYGi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación con MAP**"
      ],
      "metadata": {
        "id": "nXXVDDbTh0Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_MAP_svd(model, test_df, train_df):\n",
        "    scores = []\n",
        "    try:\n",
        "        for account_id in test_df['account_id'].unique():\n",
        "            if account_id in train_df['account_id'].values:\n",
        "                recomendaciones = generar_recomendaciones_svd(model, account_id, train_df)\n",
        "            else:\n",
        "                recomendaciones = generar_recomendaciones_cold_start(train_df)\n",
        "\n",
        "            # Verificamos si las recomendaciones son parte del contenido visto por el usuario en el test set\n",
        "            vistos = set(test_df[test_df['account_id'] == account_id]['content_id'].values)\n",
        "            y_true = [1 if rec in vistos else 0 for rec in recomendaciones]\n",
        "            y_score = [model.predict(account_id, rec).est for rec in recomendaciones]\n",
        "\n",
        "            if len(vistos) > 0:\n",
        "                scores.append(average_precision_score(y_true, y_score))\n",
        "    except KeyError as e:\n",
        "        print(f\"Error al calcular MAP: {e}\")\n",
        "        raise\n",
        "\n",
        "    return sum(scores) / len(scores) if scores else 0\n"
      ],
      "metadata": {
        "id": "dwdnCC41M2Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular y mostrar el MAP Score\n",
        "try:\n",
        "    map_score = calcular_MAP_svd(svd, test_df, train_df)\n",
        "    print(f\"MAP Score: {map_score}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al calcular el MAP Score: {e}\")"
      ],
      "metadata": {
        "id": "bdlwyFRYZC0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión**"
      ],
      "metadata": {
        "id": "zGqPZnNelm4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este proyecto, se desarrolló un sistema de recomendación empleando la factorización de matrices (SVD) sobre un conjunto de datos real de Telecom. El propósito fue generar 20 recomendaciones personalizadas para cada usuario, asegurando que no incluyeran contenido ya visualizado y que abarcaran también a aquellos usuarios sin historial previo (usuarios en cold start). La precisión del sistema se evaluó utilizando la métrica MAP, que permitió analizar la efectividad de las recomendaciones. Aunque el MAP Score obtenido fue bajo, esto refleja los desafíos inherentes a la generación de recomendaciones precisas en un escenario sin ratings explícitos y con restricciones en los datos. El proyecto pone de manifiesto la capacidad para aplicar técnicas avanzadas de recomendación y subraya la importancia de la optimización continua en sistemas de recomendación complejos."
      ],
      "metadata": {
        "id": "TNoQ1fkWl5uY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}